{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "item-based classification correction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehZlgJZpFRjP"
      },
      "source": [
        "#@title Step 1: Imports\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import torch\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt  \n",
        "import os                       \n",
        "import sklearn.datasets         \n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import json\n",
        "import copy\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSx9UCigFq-8",
        "outputId": "c11f1f6f-fcdd-4ca5-91d8-0838a220ca1f"
      },
      "source": [
        "#@title Step 2: Download data\n",
        "!pip install kaggle\n",
        "\n",
        "!mkdir ~/.kaggle\n",
        "\n",
        "kaggle_username = \"wmd0701\" #@param {type:\"string\"}\n",
        "kaggle_api_key = \"8f525bc765511d324f8509b938d7f39c\" #@param {type:\"string\"}\n",
        "\n",
        "assert len(kaggle_username) > 0 and len(kaggle_api_key) > 0\n",
        "\n",
        "api_token = {\"username\": kaggle_username,\"key\": kaggle_api_key}\n",
        "\n",
        "with open('kaggle.json', 'w') as file:\n",
        "    json.dump(api_token, file)\n",
        "\n",
        "!mv kaggle.json ~/.kaggle/kaggle.json\n",
        "\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle competitions download -c cil-collaborative-filtering-2021\n",
        "\n",
        "!unzip data_train.csv.zip \n",
        "!unzip sampleSubmission.csv.zip\n",
        "print()\n",
        "\n",
        "number_of_users, number_of_movies = (10000, 1000)\n",
        "\n",
        "data_pd = pd.read_csv('data_train.csv')\n",
        "print(data_pd.head(5))\n",
        "print('\\nShape', data_pd.shape)\n",
        "\n",
        "submission_pd = pd.read_csv('sampleSubmission.csv.zip')\n",
        "print(\"\\n\\n\",submission_pd.head(5))\n",
        "print(\"\\nShape:\", submission_pd.shape)\n",
        "print(\"\\n\\nSummary:\", np.unique(data_pd.Prediction.values, return_counts=True))\n",
        "\n",
        "sparsity = 1.0 - data_pd.shape[0] / (number_of_users * number_of_movies)\n",
        "print(\"\\nSparsity:\", sparsity)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.41.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.5.30)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (5.0.2)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n",
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4)\n",
            "Downloading data_train.csv.zip to /content\n",
            "  0% 0.00/3.33M [00:00<?, ?B/s]\n",
            "100% 3.33M/3.33M [00:00<00:00, 112MB/s]\n",
            "Downloading sampleSubmission.csv.zip to /content\n",
            "  0% 0.00/2.92M [00:00<?, ?B/s]\n",
            "100% 2.92M/2.92M [00:00<00:00, 200MB/s]\n",
            "Archive:  data_train.csv.zip\n",
            "  inflating: data_train.csv          \n",
            "Archive:  sampleSubmission.csv.zip\n",
            "  inflating: sampleSubmission.csv    \n",
            "\n",
            "       Id  Prediction\n",
            "0  r44_c1           4\n",
            "1  r61_c1           3\n",
            "2  r67_c1           4\n",
            "3  r72_c1           3\n",
            "4  r86_c1           5\n",
            "\n",
            "Shape (1176952, 2)\n",
            "\n",
            "\n",
            "         Id  Prediction\n",
            "0   r37_c1           3\n",
            "1   r73_c1           3\n",
            "2  r156_c1           3\n",
            "3  r160_c1           3\n",
            "4  r248_c1           3\n",
            "\n",
            "Shape: (1176952, 2)\n",
            "\n",
            "\n",
            "Summary: (array([1, 2, 3, 4, 5]), array([ 43508,  99180, 274327, 324700, 435237]))\n",
            "\n",
            "Sparsity: 0.8823048\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVd1Q8gbGEQG",
        "outputId": "616fe9fa-a8b4-4bf8-f82e-a553688ef51b"
      },
      "source": [
        "#@title Step 3: Split data\n",
        "\n",
        "train_size = 0.9\n",
        "\n",
        "train_pd, test_pd = train_test_split(data_pd, train_size=train_size, random_state=0)\n",
        "print(train_pd.shape)\n",
        "print(test_pd.shape)\n",
        "\n",
        "def extract_users_items_predictions(data_pd):\n",
        "    users, movies = \\\n",
        "        [np.squeeze(arr) for arr in np.split(data_pd.Id.str.extract('r(\\d+)_c(\\d+)').values.astype(int) - 1, 2, axis=-1)]\n",
        "    predictions = data_pd.Prediction.values\n",
        "    return users, movies, predictions"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1059256, 2)\n",
            "(117696, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gWksI4KdGvk5",
        "outputId": "a8d2696c-7477-4217-ca74-f46dacfa4fbd"
      },
      "source": [
        "#@title Step 4: Use GPU if available\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Device is {device}.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device is cuda.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxmdxkkNwlr2",
        "outputId": "e9217b99-cbb8-4659-bca9-a4ee95089ba6"
      },
      "source": [
        "#@title Step 5: Classifier - create data loaders\n",
        "batch_size = 64\n",
        "\n",
        "train_users, train_movies, train_predictions = extract_users_items_predictions(train_pd)\n",
        "test_users, test_movies, test_predictions = extract_users_items_predictions(test_pd)\n",
        "all_users, all_movies, all_predictions = extract_users_items_predictions(data_pd)\n",
        "\n",
        "# class encodes are 01234, not 12345\n",
        "train_predictions = train_predictions - 1\n",
        "test_predictions  = test_predictions  - 1\n",
        "all_predictions   = all_predictions   - 1\n",
        "\n",
        "# datasets and data loaders for training/validation\n",
        "train_users_  = torch.tensor(train_users, device=device).int()\n",
        "train_movies_ = torch.tensor(train_movies, device=device).int()\n",
        "train_predictions_ = torch.tensor(train_predictions, device=device).long()\n",
        "test_users_  = torch.tensor(test_users, device=device).int()\n",
        "test_movies_ = torch.tensor(test_movies, device=device).int()\n",
        "test_predictions_ = torch.tensor(test_predictions, device=device).long()\n",
        "train_set = torch.utils.data.TensorDataset(train_users_, train_movies_, train_predictions_)\n",
        "test_set  = torch.utils.data.TensorDataset(test_users_ , test_movies_ , test_predictions_)\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "test_loader  = torch.utils.data.DataLoader(test_set , batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# datasets and data loaders for the whole task\n",
        "all_users_  = torch.tensor(all_users, device=device).int()\n",
        "all_movies_ = torch.tensor(all_movies, device=device).int()\n",
        "all_predictions_ = torch.tensor(all_predictions, device=device).long()\n",
        "all_set = torch.utils.data.TensorDataset(all_users_, all_movies_, all_predictions_)\n",
        "all_loader = torch.utils.data.DataLoader(all_set , batch_size=batch_size, shuffle=True)\n",
        "\n",
        "print(len(train_loader))\n",
        "print(len(test_loader))\n",
        "print(len(all_loader))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16551\n",
            "1839\n",
            "18390\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5vad0gKHJth"
      },
      "source": [
        "#@title Step 6: Classifier - MLP class\n",
        "\n",
        "class MLP(torch.nn.Module):\n",
        "    def __init__(self, number_of_users, number_of_movies, embedding_size=64):\n",
        "        super().__init__()\n",
        "        self.embedding_layer_users = torch.nn.Embedding(number_of_users, embedding_size)\n",
        "        self.embedding_layer_movies = torch.nn.Embedding(number_of_movies, embedding_size)\n",
        "        \n",
        "        self.feed_forward = torch.nn.Sequential(\n",
        "            torch.nn.Linear(in_features=embedding_size*2, out_features=embedding_size),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(in_features=embedding_size, out_features=int(embedding_size/2)),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(in_features=int(embedding_size/2), out_features=int(embedding_size/4)),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(in_features=int(embedding_size/4), out_features=5),\n",
        "        )\n",
        "        \n",
        "    def forward(self, users, movies):\n",
        "        users_embedding = self.embedding_layer_users(users)\n",
        "        movies_embedding = self.embedding_layer_movies(movies)\n",
        "        output = torch.cat([users_embedding, movies_embedding], dim=1)\n",
        "        output = self.feed_forward(output)\n",
        "        return output\n",
        "\n",
        "# help function for calculating accuracy\n",
        "def multi_acc(y_pred, y_true):\n",
        "    y_pred_softmax = torch.log_softmax(y_pred, dim = 1)\n",
        "    _, y_pred_tags = torch.max(y_pred_softmax, dim = 1)    \n",
        "    \n",
        "    correct_pred = (y_pred_tags == y_true).float()\n",
        "    return correct_pred.sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqHjs9nNHS9R",
        "outputId": "224e02a2-6884-47e4-b53a-69188c0ab74a"
      },
      "source": [
        "#@title Step 7: Classifier - initialize\n",
        "\n",
        "def weights_init(m):\n",
        "    if isinstance(m, torch.nn.Embedding):\n",
        "        torch.nn.init.xavier_uniform_(m.weight.data)\n",
        "\n",
        "model_MLP = MLP(number_of_users, number_of_movies, embedding_size=64)\n",
        "\n",
        "# put the model in the device memory\n",
        "model_MLP = model_MLP.to(device)\n",
        "\n",
        "# count total number of parameters including non trainable\n",
        "total_params_count = sum(p.numel() for p in model_MLP.parameters())\n",
        "# count total trainable parameters\n",
        "trainable_params_count = sum(p.numel() for p in model_MLP.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"Total number of trainable parameters: {total_params_count}\")\n",
        "print(f\"Number of trainable parameters: {trainable_params_count}\")\n",
        "print(model_MLP.apply(weights_init))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of trainable parameters: 714949\n",
            "Number of trainable parameters: 714949\n",
            "MLP(\n",
            "  (embedding_layer_users): Embedding(10000, 64)\n",
            "  (embedding_layer_movies): Embedding(1000, 64)\n",
            "  (feed_forward): Sequential(\n",
            "    (0): Linear(in_features=128, out_features=64, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=64, out_features=32, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=32, out_features=16, bias=True)\n",
            "    (5): ReLU()\n",
            "    (6): Linear(in_features=16, out_features=5, bias=True)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RB31AjW9HlTe"
      },
      "source": [
        "#@title Step 8: Classifier - loss and optimier\n",
        "\n",
        "loss_func = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "learning_rate  = 3e-4\n",
        "regularization = 5e-5\n",
        "optimizer = torch.optim.Adam(model_MLP.parameters(), lr=learning_rate, weight_decay=regularization)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAJkmyFDIVIw"
      },
      "source": [
        "#@title Step 9: Classifier - training and validation\n",
        "\n",
        "\n",
        "# set seed to make result reproducible\n",
        "torch.manual_seed(0)\n",
        "\n",
        "# number of epochs\n",
        "n_epochs = 11\n",
        "\n",
        "# whether to train with 90% data and validate with 10%,\n",
        "# or to train with 100% data\n",
        "validate = True\n",
        "\n",
        "if validate:\n",
        "    t_loader = train_loader\n",
        "else:\n",
        "    t_loader = all_loader\n",
        "  \n",
        "\n",
        "step = 0\n",
        "for epoch in range(n_epochs):\n",
        "    \n",
        "    # train model\n",
        "    model_MLP.train()\n",
        "    total_loss_train, total_accuracy_train = 0., 0.\n",
        "    for i, (user, movie, rating) in enumerate(t_loader):\n",
        "        \n",
        "        optimizer.zero_grad()   \n",
        "        output = model_MLP(user, movie) \n",
        "        loss = loss_func(output, rating) \n",
        "        loss.backward()\n",
        "        optimizer.step() \n",
        "        total_loss_train += loss.item()\n",
        "        \n",
        "        if validate:\n",
        "            total_accuracy_train += multi_acc(output, rating)\n",
        "\n",
        "        if step % 5000 == 0:\n",
        "            print('[Epoch %03d] - Step %04d> train loss: %.4f' % (epoch, step, loss.item()))\n",
        "        \n",
        "        step += 1\n",
        "\n",
        "    # validate model\n",
        "    if validate:\n",
        "        model_MLP.eval()\n",
        "        total_loss_test, total_accuracy_test = 0., 0.\n",
        "        with torch.no_grad():\n",
        "            for i, (user, movie, rating) in enumerate(test_loader):\n",
        "                output = model_MLP(user, movie)\n",
        "                loss = loss_func(output, rating)\n",
        "                total_loss_test += loss.item()\n",
        "                # total_accuracy_test += (output.round() == rating).float().sum()\n",
        "                total_accuracy_test += multi_acc(output, rating)\n",
        "\n",
        "                if step % 5000 == 0:\n",
        "                    print('[Epoch %03d] - Step %04d> test  loss: %.4f' % (epoch, step, loss.item()))      \n",
        "\n",
        "\n",
        "\n",
        "    total_loss_train /= len(t_loader)\n",
        "    print('[Epoch %03d] - > avg train loss: %.4f' % (epoch, total_loss_train))\n",
        "\n",
        "    if validate:\n",
        "        total_loss_test  /= len(test_loader)\n",
        "        total_accuracy_train /= (len(t_loader)*batch_size)\n",
        "        total_accuracy_test  /= (len(test_loader)*batch_size)\n",
        "        \n",
        "        print('[Epoch %03d] - > avg test  loss: %.4f' % (epoch, total_loss_test))\n",
        "        print('[Epoch %03d] - > avg train accu: %.4f' % (epoch, total_accuracy_train)) \n",
        "        print('[Epoch %03d] - > avg test  accu: %.4f' % (epoch, total_accuracy_test)) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4SF0HP2Wza-g",
        "outputId": "76a18f57-3985-4828-8b11-c94ec2a452d8"
      },
      "source": [
        "#@title Step 10: Regressor - create data loaders\n",
        "batch_size = 64\n",
        "\n",
        "train_users, train_movies, train_predictions = extract_users_items_predictions(train_pd)\n",
        "test_users, test_movies, test_predictions = extract_users_items_predictions(test_pd)\n",
        "all_users, all_movies, all_predictions = extract_users_items_predictions(data_pd)\n",
        "\n",
        "# datasets and data loaders for training/validation\n",
        "train_users_  = torch.tensor(train_users, device=device).int()\n",
        "train_movies_ = torch.tensor(train_movies, device=device).int()\n",
        "train_predictions_ = torch.tensor(train_predictions, device=device).float()\n",
        "test_users_  = torch.tensor(test_users, device=device).int()\n",
        "test_movies_ = torch.tensor(test_movies, device=device).int()\n",
        "test_predictions_ = torch.tensor(test_predictions, device=device).float()\n",
        "train_set = torch.utils.data.TensorDataset(train_users_, train_movies_, train_predictions_)\n",
        "test_set  = torch.utils.data.TensorDataset(test_users_ , test_movies_ , test_predictions_)\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "test_loader  = torch.utils.data.DataLoader(test_set , batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# datasets and data loaders for the whole task\n",
        "all_users_  = torch.tensor(all_users, device=device).int()\n",
        "all_movies_ = torch.tensor(all_movies, device=device).int()\n",
        "all_predictions_ = torch.tensor(all_predictions, device=device).float()\n",
        "all_set = torch.utils.data.TensorDataset(all_users_, all_movies_, all_predictions_)\n",
        "all_loader = torch.utils.data.DataLoader(all_set , batch_size=batch_size, shuffle=True)\n",
        "\n",
        "print(len(train_loader))\n",
        "print(len(test_loader))\n",
        "print(len(all_loader))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16551\n",
            "1839\n",
            "18390\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FususUsZw8LR"
      },
      "source": [
        "#@title Step 11: Regressor - SVD++ class\n",
        "\n",
        "class SVDpp(torch.nn.Module):\n",
        "  def __init__(self, number_of_users=number_of_users, number_of_movies=number_of_movies, rank=20, biased=True):\n",
        "    super().__init__()\n",
        "    self.biased = biased\n",
        "    self.rank = rank\n",
        "    self.user_matrix   = torch.nn.Embedding(number_of_users , self.rank)\n",
        "    self.movie_matrix  = torch.nn.Embedding(number_of_movies, self.rank)\n",
        "    self.user_biases   = torch.nn.Embedding(number_of_users , 1)\n",
        "    self.movie_biases  = torch.nn.Embedding(number_of_movies, 1)\n",
        "    self.global_biases = torch.nn.Embedding(1, 1)\n",
        "\n",
        "    # constants\n",
        "    self.zero = torch.tensor(0).to(device)\n",
        "    self.register_buffer('const_zero', self.zero)\n",
        "\n",
        "\n",
        "  def forward(self, user, movie):\n",
        "    pred  = (self.user_matrix(user) * self.movie_matrix(movie)).sum(1, keepdim=True)\n",
        "    if self.biased:\n",
        "      pred += self.user_biases(user) + self.movie_biases(movie) + self.global_biases(self.zero)\n",
        "    \n",
        "    return pred.squeeze()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qe6U2NFCxD-E",
        "outputId": "371d39a9-521a-4c99-dac4-4c9791d9d6bf"
      },
      "source": [
        "#@title Step 12: Regressor - initialize\n",
        "model_SVD = SVDpp(number_of_users, number_of_movies, rank=20, biased=True)\n",
        "\n",
        "# put the model in the device memory\n",
        "model_SVD = model_SVD.to(device)\n",
        "\n",
        "# count total number of parameters including non trainable\n",
        "total_params_count = sum(p.numel() for p in model_SVD.parameters())\n",
        "# count total trainable parameters\n",
        "trainable_params_count = sum(p.numel() for p in model_SVD.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"Total number of trainable parameters: {total_params_count}\")\n",
        "print(f\"Number of trainable parameters: {trainable_params_count}\")\n",
        "print(model_SVD.apply(weights_init))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of trainable parameters: 231001\n",
            "Number of trainable parameters: 231001\n",
            "SVDpp(\n",
            "  (user_matrix): Embedding(10000, 20)\n",
            "  (movie_matrix): Embedding(1000, 20)\n",
            "  (user_biases): Embedding(10000, 1)\n",
            "  (movie_biases): Embedding(1000, 1)\n",
            "  (global_biases): Embedding(1, 1)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWNJZUBxxw6s"
      },
      "source": [
        "#@title Step 13: Regressor - loss and optimizer\n",
        "\n",
        "loss_func = torch.nn.MSELoss()\n",
        "\n",
        "learning_rate  = 3e-4\n",
        "regularization = 5e-5\n",
        "\n",
        "optimizer = torch.optim.Adam(model_SVD.parameters(), lr=learning_rate, weight_decay=regularization)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ie0BHHrAWB6h"
      },
      "source": [
        "#@title Step 14: Regressor - training and validation\n",
        "\n",
        "# set seed to make result reproducible\n",
        "torch.manual_seed(0)\n",
        "\n",
        "# number of epochs\n",
        "n_epochs = 23\n",
        "\n",
        "# whether to train with 90% data and validate with 10%,\n",
        "# or to train with 100% data\n",
        "validate = True\n",
        "\n",
        "if validate:\n",
        "    t_loader = train_loader\n",
        "else:\n",
        "    t_loader = all_loader\n",
        "  \n",
        "\n",
        "step = 0\n",
        "for epoch in range(n_epochs):\n",
        "    \n",
        "    # train model\n",
        "    model_SVD.train()\n",
        "    total_loss_train, total_accuracy_train = 0., 0.\n",
        "    for i, (user, movie, rating) in enumerate(t_loader):\n",
        "       \n",
        "        optimizer.zero_grad()   \n",
        "        output = model_SVD(user, movie) \n",
        "        loss = loss_func(output, rating) \n",
        "        loss.backward() \n",
        "        optimizer.step() \n",
        "\n",
        "        total_loss_train += loss.item()\n",
        "        \n",
        "        # measure accuracy\n",
        "        if validate:\n",
        "            total_accuracy_train += (output.round() == rating).float().sum()\n",
        "        \n",
        "        if step % 5000 == 0:\n",
        "            print('[Epoch %03d] - Step %04d> train loss: %.4f' % (epoch, step, loss.item()))\n",
        "        step += 1\n",
        "\n",
        "\n",
        "    # validate model\n",
        "    if validate:\n",
        "        model_SVD.eval()\n",
        "        total_loss_test, total_accuracy_test = 0., 0.\n",
        "        with torch.no_grad():\n",
        "            for i, (user, movie, rating) in enumerate(test_loader):\n",
        "                output = model_SVD(user, movie)\n",
        "                loss = loss_func(output, rating)\n",
        "\n",
        "                total_loss_test += loss.item()\n",
        "                total_accuracy_test += (output.round() == rating).float().sum()\n",
        "\n",
        "                if step % 5000 == 0:\n",
        "                    print('[Epoch %03d] - Step %04d> test  loss: %.4f' % (epoch, step, loss.item()))      \n",
        "\n",
        "\n",
        "\n",
        "    total_loss_train /= len(t_loader)\n",
        "    print('[Epoch %03d] - > avg train loss: %.4f' % (epoch, total_loss_train)) \n",
        "\n",
        "    if validate:\n",
        "        total_loss_test  /= len(test_loader) \n",
        "        total_accuracy_train /= (len(t_loader)*batch_size)\n",
        "        total_accuracy_test  /= (len(test_loader)*batch_size)   \n",
        "    \n",
        "        print('[Epoch %03d] - > avg test  loss: %.4f' % (epoch, total_loss_test))\n",
        "        print('[Epoch %03d] - > avg train accu: %.4f' % (epoch, total_accuracy_train)) \n",
        "        print('[Epoch %03d] - > avg test  accu: %.4f' % (epoch, total_accuracy_test)) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qmx1SiFrykBi"
      },
      "source": [
        "#@title Step 15: Combo - class\n",
        "\n",
        "class Combo(torch.nn.Module):\n",
        "  def __init__(self, regressor=model_SVD, classifier=model_MLP):\n",
        "    super().__init__()\n",
        "    \n",
        "    # regressor and classifier are pre-trained\n",
        "    self.regressor  = copy.deepcopy(regressor)\n",
        "    self.classifier = copy.deepcopy(classifier)\n",
        "\n",
        "    # freeze pre-trained models\n",
        "    for param in self.regressor.parameters():\n",
        "\t\t    param.requires_grad = False\n",
        "    for param in self.classifier.parameters():\n",
        "\t\t    param.requires_grad = False\n",
        "\n",
        "    # alpha\n",
        "    self.alpha = torch.nn.Embedding(number_of_movies, 1)\n",
        "    torch.nn.init.zeros_(self.alpha.weight)\n",
        "\n",
        "    # constants\n",
        "    self.one = torch.tensor(1.0)\n",
        "    self.register_buffer('const_one', self.one)\n",
        "    \n",
        "  def forward(self, user, movie):\n",
        "    _, classifier_output  = torch.max(self.classifier(user, movie), dim = 1)\n",
        "    \n",
        "    # classifier outputs are categorical values 0~4, we need 1~5\n",
        "    classifier_output = classifier_output + self.one\n",
        "    \n",
        "    regressor_output = self.regressor(user, movie)\n",
        "    \n",
        "    alpha = self.alpha(movie).squeeze()\n",
        "    \n",
        "    output = (self.one - alpha) * regressor_output + alpha * classifier_output\n",
        "    output = torch.squeeze(output)\n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7D4C9_FHI2H",
        "outputId": "be3e19e6-953b-4245-b8d8-d4f3abf19966"
      },
      "source": [
        "#@title Step 16: Combo - initialize\n",
        "\n",
        "model_combo = Combo(regressor=model_SVD, classifier=model_MLP)\n",
        "\n",
        "# put the model in the device memory\n",
        "model_combo = model_combo.to(device)\n",
        "\n",
        "# count total number of parameters including non trainable\n",
        "total_params_count = sum(p.numel() for p in model_combo.parameters())\n",
        "# count total trainable parameters\n",
        "trainable_params_count = sum(p.numel() for p in model_combo.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"Total number of trainable parameters: {total_params_count}\")\n",
        "print(f\"Number of trainable parameters: {trainable_params_count}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of trainable parameters: 946950\n",
            "Number of trainable parameters: 1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWfQ63IZHaQk"
      },
      "source": [
        "#@title Step 17: Combo - loss and optimizer\n",
        "\n",
        "loss_func = torch.nn.MSELoss()\n",
        "\n",
        "\n",
        "learning_rate  = 1e-5\n",
        "regularization = 1e-5\n",
        "\n",
        "optimizer = torch.optim.Adam(model_combo.parameters(), lr=learning_rate, weight_decay=regularization)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WERlayZH2yp"
      },
      "source": [
        "#@title Step 18: Combo - training and validation\n",
        "\n",
        "# set seed to make result reproducible\n",
        "torch.manual_seed(0)\n",
        "\n",
        "# number of epochs\n",
        "n_epochs = 30\n",
        "\n",
        "# whether to train with 90% data and validate with 10%,\n",
        "# or to train with 100% data\n",
        "validate = True\n",
        "\n",
        "if validate:\n",
        "    t_loader = train_loader\n",
        "else:\n",
        "    t_loader = all_loader\n",
        "  \n",
        "\n",
        "step = 0\n",
        "for epoch in range(n_epochs):\n",
        "    \n",
        "    # train model\n",
        "    model_combo.train()\n",
        "    total_loss_train, total_accuracy_train = 0., 0.\n",
        "    for i, (user, movie, rating) in enumerate(t_loader):\n",
        "        \n",
        "        optimizer.zero_grad()   \n",
        "        output = model_combo(user, movie) \n",
        "        loss = loss_func(output, rating) \n",
        "        loss.backward()\n",
        "        optimizer.step() \n",
        "        total_loss_train += loss.item()\n",
        "        \n",
        "        # output is 0.5 --- 5.5\n",
        "        if validate:\n",
        "            total_accuracy_train += (output.round() == rating).float().sum()\n",
        "        \n",
        "        if step % 5000 == 0:\n",
        "            print('[Epoch %03d] - Step %04d> train loss: %.4f' % (epoch, step, loss.item()))\n",
        "        step += 1\n",
        "\n",
        "    # validate model\n",
        "    if validate:\n",
        "        model_combo.eval()\n",
        "        total_loss_test, total_accuracy_test = 0., 0.\n",
        "        with torch.no_grad():\n",
        "            for i, (user, movie, rating) in enumerate(test_loader):\n",
        "                \n",
        "                output = model_combo(user, movie)\n",
        "                loss = loss_func(output, rating)\n",
        "                total_loss_test += loss.item()\n",
        "                total_accuracy_test += (output.round() == rating).float().sum()\n",
        "\n",
        "                if step % 5000 == 0:\n",
        "                    print('[Epoch %03d] - Step %04d> test  loss: %.4f' % (epoch, step, loss.item()))      \n",
        "\n",
        "\n",
        "\n",
        "    total_loss_train /= len(t_loader)\n",
        "    print('[Epoch %03d] - > avg train loss: %.4f' % (epoch, total_loss_train)) \n",
        "\n",
        "    if validate:\n",
        "        total_loss_test  /= len(test_loader)\n",
        "        total_accuracy_train /= (len(t_loader)*batch_size)\n",
        "        total_accuracy_test  /= (len(test_loader)*batch_size)\n",
        "        \n",
        "        print('[Epoch %03d] - > avg test  loss: %.4f' % (epoch, total_loss_test))\n",
        "        print('[Epoch %03d] - > avg train accu: %.4f' % (epoch, total_accuracy_train)) \n",
        "        print('[Epoch %03d] - > avg test  accu: %.4f' % (epoch, total_accuracy_test)) "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}